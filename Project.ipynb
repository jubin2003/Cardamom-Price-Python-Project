{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler started. The script will run every 24 hours.\n"
     ]
    }
   ],
   "source": [
    "import requests  # For sending HTTP requests to websites\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import csv  # For saving data into CSV files\n",
    "from datetime import datetime  # For handling dates and timestamps\n",
    "import schedule  # For scheduling the script to run at a specific time\n",
    "import time  # For adding delays in execution\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.indianspices.com/marketing/price/domestic/daily-price.html'\n",
    "\n",
    "# Headers to make the request look like it's coming from a real web browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "def scrape_data():\n",
    "    print(\"Scraping data...\")\n",
    "\n",
    "    # Send a request to the website and get the response\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful (status code 200 means success)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return  # Stop execution if the page can't be retrieved\n",
    "\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Function to extract data from the Small Cardamom table\n",
    "    def process_small_cardamom_table(table):\n",
    "        rows = table.find_all('tr')  # Get all table rows\n",
    "        if len(rows) < 3:\n",
    "            return [], []  # Return empty lists if not enough data is found\n",
    "        \n",
    "        # Define the column headers we need\n",
    "        desired_headers = ['Date', 'Auctioneer', 'No. of Lots', 'Total Qty Arrived', 'Qty Sold', 'Max Price', 'Avg Price']\n",
    "        data = []\n",
    "        \n",
    "        for row in rows[2:]:  # Skip the first two rows (headers and unwanted data)\n",
    "            cols = row.find_all('td')  # Get all columns in the row\n",
    "            if len(cols) >= 8:\n",
    "                data.append([\n",
    "                    cols[1].get_text(strip=True),\n",
    "                    cols[2].get_text(strip=True),\n",
    "                    cols[3].get_text(strip=True),\n",
    "                    cols[4].get_text(strip=True),\n",
    "                    cols[5].get_text(strip=True),\n",
    "                    cols[6].get_text(strip=True),\n",
    "                    cols[7].get_text(strip=True)\n",
    "                ])\n",
    "        \n",
    "        return desired_headers, data\n",
    "\n",
    "    # Function to extract data from the Large Cardamom table\n",
    "    def process_large_cardamom_table(table):\n",
    "        rows = table.find_all('tr')\n",
    "        if len(rows) < 3:\n",
    "            return [], []\n",
    "        \n",
    "        # Define the column headers we need\n",
    "        desired_headers = ['Date', 'Market', 'Type', 'Price']\n",
    "        data = []\n",
    "        \n",
    "        for row in rows[2:]:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) >= 5:\n",
    "                data.append([\n",
    "                    cols[1].get_text(strip=True),\n",
    "                    cols[2].get_text(strip=True),\n",
    "                    cols[3].get_text(strip=True),\n",
    "                    cols[4].get_text(strip=True)\n",
    "                ])\n",
    "        \n",
    "        return desired_headers, data\n",
    "\n",
    "    # Locate the Small Cardamom section on the webpage\n",
    "    small_heading = soup.find('h2', string=lambda t: t and 'Small Cardamom' in t)\n",
    "    small_table = small_heading.find_next('table') if small_heading else None\n",
    "    \n",
    "    # Locate the Large Cardamom section on the webpage\n",
    "    large_heading = soup.find('h2', string=lambda t: t and 'Large Cardamom' in t)\n",
    "    large_table = large_heading.find_next('table') if large_heading else None\n",
    "    \n",
    "    # Extract data from the Small Cardamom table\n",
    "    small_headers, small_data = process_small_cardamom_table(small_table) if small_table else ([], [])\n",
    "    \n",
    "    # Extract data from the Large Cardamom table\n",
    "    large_headers, large_data = process_large_cardamom_table(large_table) if large_table else ([], [])\n",
    "\n",
    "    # Get the current date in YYYYMMDD format to use in filenames\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "    # Save Small Cardamom data to a CSV file\n",
    "    if small_headers and small_data:\n",
    "        small_filename = f\"small_cardamom_prices_{current_date}.csv\"\n",
    "        with open(small_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(small_headers)  # Write headers\n",
    "            writer.writerows(small_data)  # Write data rows\n",
    "        print(f\"Small Cardamom data saved to {small_filename}\")\n",
    "\n",
    "    # Save Large Cardamom data to a CSV file\n",
    "    if large_headers and large_data:\n",
    "        large_filename = f\"large_cardamom_prices_{current_date}.csv\"\n",
    "        with open(large_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(large_headers)  # Write headers\n",
    "            writer.writerows(large_data)  # Write data rows\n",
    "        print(f\"Large Cardamom data saved to {large_filename}\")\n",
    "\n",
    "    # Print some sample data from Small Cardamom\n",
    "    if small_data:\n",
    "        print(\"\\nSample Small Cardamom Data:\")\n",
    "        for row in small_data[:2]:  # Show first two rows only\n",
    "            print(row)\n",
    "    \n",
    "    # Print some sample data from Large Cardamom\n",
    "    if large_data:\n",
    "        print(\"\\nSample Large Cardamom Data:\")\n",
    "        for row in large_data[:2]:  # Show first two rows only\n",
    "            print(row)\n",
    "\n",
    "# **Schedule the script to run every 24 hours at a fixed time**\n",
    "schedule.every().day.at(\"08:00\").do(scrape_data)  # Runs daily at 08:00 AM\n",
    "\n",
    "print(\"Scheduler started. The script will run every 24 hours.\")\n",
    "\n",
    "# Keep the script running indefinitely and check every minute if it's time to run\n",
    "while True:\n",
    "    schedule.run_pending()  # Check if any scheduled task is due\n",
    "    time.sleep(60)  # Wait for 60 seconds before checking again\n",
    "    # Note: This will keep the script running indefinitely. You can stop it with Ctrl+C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
